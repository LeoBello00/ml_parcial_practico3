{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_fraud = pd.read_csv('../data/data_preprocessed/data_fraud2.csv')\n",
    "data_fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_fraud_sampled = data_fraud.sample(frac=1, random_state=42)\n",
    "\n",
    "X, y = data_fraud_sampled.drop('isFraud', axis=1), data_fraud_sampled['isFraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ada = ADASYN(random_state=42)\n",
    "X_train_res, y_train_res = ada.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Original train dataset shape {}'.format(Counter(y_train)))\n",
    "print('Resampled train dataset shape {}'.format(Counter(y_train_res)))\n",
    "print('Test dataset shape {}'.format(Counter(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nb_col = X_train.shape[1]\n",
    "\n",
    "dict_models = [\n",
    "    {\n",
    "        'name_clf' : 'Random Forest',\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'scalers' : {\n",
    "            'scaler': [None],\n",
    "        },\n",
    "        'grid' : {\n",
    "            'model__max_depth': list(range(1, nb_col + 1, 5)),\n",
    "        },    \n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from utils import model_evaluation_clf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Model\", \"CPU time\", \"Accuracy\", \"Precision\", \"Recall\", \"f1-score\", \"AUC\"])\n",
    "models = {}\n",
    "nb_res = 0\n",
    "\n",
    "for i, dict_clf in enumerate(dict_models):\n",
    "    model_name = dict_clf['name_clf']\n",
    "    print(f'Training {model_name}...')\n",
    "\n",
    "    model = dict_clf['model']\n",
    "\n",
    "    steps = [\n",
    "        ('scaler', None),\n",
    "        ('model', model),\n",
    "    ]\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    param_grid = {\n",
    "        **dict_clf['scalers'],\n",
    "        **dict_clf['grid']\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "    start_time = time()\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(f\"Best params {model_name}: \\n{clf.best_params_}\")\n",
    "\n",
    "    eval = model_evaluation_clf(clf, X_test, y_test)\n",
    "    \n",
    "    models[model_name] = clf\n",
    "\n",
    "    results.loc[nb_res] = [model_name, round(end_time - start_time, 1), eval['accuracy'], eval['precision'], eval['recall'], eval['f1'], eval['roc_auc']]\n",
    "    nb_res += 1\n",
    "\n",
    "    print(f\"CPU Time: {round(end_time - start_time, 1)}s\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values(by='AUC', ascending=False)\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "for i, model_name in enumerate(models.keys()):\n",
    "    model = models[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axs[i])\n",
    "    axs[i].set_title(model_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "for model_name in models.keys():\n",
    "    model = models[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    ax.plot(fpr, tpr, label=model_name)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
